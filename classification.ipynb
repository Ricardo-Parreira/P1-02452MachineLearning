{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classification\n",
   "id": "eae5ed7e2ef1c44a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T16:30:25.342563Z",
     "start_time": "2025-11-09T16:30:21.315448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Load the body dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Map famhist to numeric\n",
    "if \"famhist\" in df.columns:\n",
    "    df[\"famhist\"] = df[\"famhist\"].map({\"Absent\": 0, \"Present\": 1})\n",
    "\n",
    "# Fix skewed variables\n",
    "for col in [\"tobacco\", \"alcohol\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = np.log1p(df[col])\n",
    "\n",
    "# Split the data frame into features and labels\n",
    "X = df.drop(columns=[\"chd\"])\n",
    "y = df[\"chd\"]\n",
    "\n",
    "#parameter ranges\n",
    "lambda_range = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "C_range = [1.0 / lam for lam in lambda_range]  # sklearn uses C = 1/Î»\n",
    "k_range = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "\n",
    "param_grid_log = {\"clf__C\": C_range}\n",
    "param_grid_knn = {\"clf__n_neighbors\": k_range}\n",
    "\n",
    "# 2-layer CV\n",
    "def two_layer_cv(X, y, outer_folds=10, inner_folds=10, random_state=42):\n",
    "    CV_outer = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_results = []\n",
    "    for i, (train_outer_idx, test_outer_idx) in enumerate(CV_outer.split(X, y)):\n",
    "        # Split X and y into training/testing data for this outer fold\n",
    "        # Name them X_train_outer, X_test_outer, y_train_outer, y_test_outer\n",
    "        X_train_outer, X_test_outer = X.iloc[train_outer_idx], X.iloc[test_outer_idx]\n",
    "        y_train_outer, y_test_outer = y.iloc[train_outer_idx], y.iloc[test_outer_idx]\n",
    "\n",
    "        # baseline\n",
    "        maj_class = Counter(y_train_outer).most_common(1)[0][0]\n",
    "        y_test_pred_base = np.full_like(y_test_outer, fill_value=maj_class)\n",
    "        Etest_base = np.mean(y_test_outer != y_test_pred_base)\n",
    "\n",
    "        #logistic regression (inner loop)\n",
    "        inner_CV = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "        val_error = []\n",
    "        for C in C_range:\n",
    "            inner_err = []\n",
    "            for tr_in_idx, te_in_idx in inner_CV.split(X_train_outer, y_train_outer):\n",
    "                X_train_inner, X_test_inner = X_train_outer.iloc[tr_in_idx], X_train_outer.iloc[te_in_idx]\n",
    "                y_train_inner, y_test_inner = y_train_outer.iloc[tr_in_idx], y_train_outer.iloc[te_in_idx]\n",
    "\n",
    "                model = make_pipeline(StandardScaler(), LogisticRegression(C=C, max_iter=1000))\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_test_inner)\n",
    "                inner_err.append(np.mean(y_pred_inner != y_test_inner))\n",
    "            val_error.append(np.mean(inner_err))\n",
    "        best_idx = np.argmin(val_error)\n",
    "        C_star = C_range[best_idx]\n",
    "        lambda_star = lambda_range[best_idx]\n",
    "\n",
    "        # Retrain on full outer-train with best C, test on outer-test\n",
    "        model_log = make_pipeline(StandardScaler(), LogisticRegression(C=C_star, max_iter=1000))\n",
    "        model_log.fit(X_train_outer, y_train_outer)\n",
    "        y_pred_outer_log = model_log.predict(X_test_outer)\n",
    "        Etest_log = np.mean(y_pred_outer_log != y_test_outer)\n",
    "\n",
    "        # knn (inner loop)\n",
    "        val_error_knn = []\n",
    "        for k in k_range:\n",
    "            inner_err = []\n",
    "            for tr_in_idx, te_in_idx in inner_CV.split(X_train_outer, y_train_outer):\n",
    "                X_train_inner, X_test_inner = X_train_outer.iloc[tr_in_idx], X_train_outer.iloc[te_in_idx]\n",
    "                y_train_inner, y_test_inner = y_train_outer.iloc[tr_in_idx], y_train_outer.iloc[te_in_idx]\n",
    "\n",
    "                model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k))\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_test_inner)\n",
    "                inner_err.append(np.mean(y_pred_inner != y_test_inner))\n",
    "            val_error_knn.append(np.mean(inner_err))\n",
    "        best_idx_knn = np.argmin(val_error_knn)\n",
    "        k_star = k_range[best_idx_knn]\n",
    "\n",
    "        # Retrain on full outer-train with best k, test on outer-test\n",
    "        model_knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k_star))\n",
    "        model_knn.fit(X_train_outer, y_train_outer)\n",
    "        y_pred_outer_knn = model_knn.predict(X_test_outer)\n",
    "        Etest_knn = np.mean(y_pred_outer_knn != y_test_outer)\n",
    "\n",
    "        #save fold results\n",
    "        fold_results.append({\n",
    "            \"\": i + 1,\n",
    "            \"lambda* (log)\": lambda_star,\n",
    "            \"Etest_log\": Etest_log,\n",
    "            \"k* (knn)\": k_star,\n",
    "            \"Etest_knn\": Etest_knn,\n",
    "            \"Etest_base\": Etest_base\n",
    "        })\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "#run two-level CV\n",
    "table = two_layer_cv(X, y, outer_folds=10, inner_folds=10, random_state=1)\n",
    "table = pd.DataFrame(table)\n",
    "#display results\n",
    "summary = pd.DataFrame({\n",
    "    \"\": [\"logistic\", \"knn\", \"baseline\"],\n",
    "    \"Etest_mean\": [\n",
    "        table[\"Etest_log\"].mean(),\n",
    "        table[\"Etest_knn\"].mean(),\n",
    "        table[\"Etest_base\"].mean()\n",
    "    ],\n",
    "    \"Etest_sd\": [\n",
    "        table[\"Etest_log\"].std(ddof=1),\n",
    "        table[\"Etest_knn\"].std(ddof=1),\n",
    "        table[\"Etest_base\"].std(ddof=1)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the results\n",
    "print(\"2-layer cross-validation results per fold\")\n",
    "print(table.round(4).to_string(index=False))\n",
    "print(\"\\nSummary\")\n",
    "print(summary.round(4).to_string(index=False))"
   ],
   "id": "7557fcae4e811db5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-layer cross-validation results per fold\n",
      "    lambda* (log)  Etest_log  k* (knn)  Etest_knn  Etest_base\n",
      " 1        10.0000     0.2979        15     0.2553      0.3404\n",
      " 2       100.0000     0.2979        11     0.2128      0.3404\n",
      " 3        10.0000     0.2391        15     0.2609      0.3478\n",
      " 4        10.0000     0.3043        13     0.3261      0.3478\n",
      " 5        10.0000     0.2609        15     0.2609      0.3478\n",
      " 6         0.0001     0.3478        15     0.3913      0.3478\n",
      " 7        10.0000     0.2826        15     0.3478      0.3478\n",
      " 8        10.0000     0.2391        15     0.3043      0.3478\n",
      " 9        10.0000     0.2826        11     0.3043      0.3478\n",
      "10        10.0000     0.3043        15     0.2826      0.3478\n",
      "\n",
      "Summary\n",
      "          Etest_mean  Etest_sd\n",
      "logistic      0.2857    0.0330\n",
      "     knn      0.2946    0.0517\n",
      "baseline      0.3463    0.0031\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "fa1a16aa",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c4dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_data(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df['famhist'] = df['famhist'].map({'Present': 1, 'Absent': 0})\n",
    "    df = (df-df.mean()) / df.std()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468668c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: [-0.87863012]\n",
      "Features: ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
      "Coefficients for each feature: [[ 0.13321451  0.36438477  0.36031357  0.14263364  0.45636594  0.38864065\n",
      "  -0.26353865  0.00313647  0.66182163]]\n",
      "Logistic Regression Accuracy: 0.7338\n",
      "Logistic Regression MSE: 0.2662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def logistic_regression_classification():\n",
    "    threshold = 0.5\n",
    "\n",
    "    df = regularize_data(\"data.csv\")\n",
    "    y = (df[\"chd\"]>threshold).astype(int)\n",
    "    X = df.drop(columns=[\"chd\", \"row.names\"])\n",
    "\n",
    "\n",
    "    lambda_val = 0.1 #pode ter que ser mudado porque depende do ponto 4\n",
    "\n",
    "\n",
    "    C_val = 1 / lambda_val\n",
    "    logreg_model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=100,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    logreg_model.fit(X, y)\n",
    "\n",
    "    print(\"Bias:\", logreg_model.intercept_)\n",
    "    print(\"Features:\", X.columns.tolist())\n",
    "    print(\"Coefficients for each feature:\", logreg_model.coef_)\n",
    "\n",
    "\n",
    "    y_pred = logreg_model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    # mse\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "    print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Logistic Regression MSE: {mse:.4f}\")\n",
    "\n",
    "logistic_regression_classification()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
