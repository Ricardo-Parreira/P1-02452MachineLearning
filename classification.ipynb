{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classification\n",
   "id": "eae5ed7e2ef1c44a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T12:04:58.514169Z",
     "start_time": "2025-11-10T12:04:54.048788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import scipy.stats as st\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Load the body dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Map famhist to numeric\n",
    "if \"famhist\" in df.columns:\n",
    "    df[\"famhist\"] = df[\"famhist\"].map({\"Absent\": 0, \"Present\": 1})\n",
    "\n",
    "# Fix skewed variables\n",
    "for col in [\"tobacco\", \"alcohol\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = np.log1p(df[col])\n",
    "\n",
    "# Split the data frame into features and labels\n",
    "X = df.drop(columns=[\"chd\"])\n",
    "y = df[\"chd\"]\n",
    "\n",
    "#parameter ranges\n",
    "lambda_range = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "C_range = [1.0 / lam for lam in lambda_range]  # sklearn uses C = 1/λ\n",
    "k_range = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "\n",
    "param_grid_log = {\"clf__C\": C_range}\n",
    "param_grid_knn = {\"clf__n_neighbors\": k_range}\n",
    "\n",
    "# 2-layer CV\n",
    "def two_layer_cv(X, y, outer_folds=10, inner_folds=10, random_state=42):\n",
    "    CV_outer = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    fold_results = []\n",
    "    y_true_all = []\n",
    "    yhat_log_all = []\n",
    "    yhat_knn_all = []\n",
    "    yhat_base_all = []\n",
    "\n",
    "    for i, (train_outer_idx, test_outer_idx) in enumerate(CV_outer.split(X, y)):\n",
    "        # Split X and y into training/testing data for this outer fold\n",
    "        # Name them X_train_outer, X_test_outer, y_train_outer, y_test_outer\n",
    "        X_train_outer, X_test_outer = X.iloc[train_outer_idx], X.iloc[test_outer_idx]\n",
    "        y_train_outer, y_test_outer = y.iloc[train_outer_idx], y.iloc[test_outer_idx]\n",
    "\n",
    "        # baseline\n",
    "        maj_class = Counter(y_train_outer).most_common(1)[0][0]\n",
    "        y_test_pred_base = np.full_like(y_test_outer, fill_value=maj_class)\n",
    "        Etest_base = np.mean(y_test_outer != y_test_pred_base)\n",
    "\n",
    "        #logistic regression (inner loop)\n",
    "        inner_CV = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "        val_error = []\n",
    "        for C in C_range:\n",
    "            inner_err = []\n",
    "            for tr_in_idx, te_in_idx in inner_CV.split(X_train_outer, y_train_outer):\n",
    "                X_train_inner, X_test_inner = X_train_outer.iloc[tr_in_idx], X_train_outer.iloc[te_in_idx]\n",
    "                y_train_inner, y_test_inner = y_train_outer.iloc[tr_in_idx], y_train_outer.iloc[te_in_idx]\n",
    "\n",
    "                model = make_pipeline(StandardScaler(), LogisticRegression(C=C, max_iter=1000))\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_test_inner)\n",
    "                inner_err.append(np.mean(y_pred_inner != y_test_inner))\n",
    "            val_error.append(np.mean(inner_err))\n",
    "        best_idx = np.argmin(val_error)\n",
    "        C_star = C_range[best_idx]\n",
    "        lambda_star = lambda_range[best_idx]\n",
    "\n",
    "        # Retrain on full outer-train with best C, test on outer-test\n",
    "        model_log = make_pipeline(StandardScaler(), LogisticRegression(C=C_star, max_iter=1000))\n",
    "        model_log.fit(X_train_outer, y_train_outer)\n",
    "        y_pred_outer_log = model_log.predict(X_test_outer)\n",
    "        Etest_log = np.mean(y_pred_outer_log != y_test_outer)\n",
    "\n",
    "        # knn (inner loop)\n",
    "        val_error_knn = []\n",
    "        for k in k_range:\n",
    "            inner_err = []\n",
    "            for tr_in_idx, te_in_idx in inner_CV.split(X_train_outer, y_train_outer):\n",
    "                X_train_inner, X_test_inner = X_train_outer.iloc[tr_in_idx], X_train_outer.iloc[te_in_idx]\n",
    "                y_train_inner, y_test_inner = y_train_outer.iloc[tr_in_idx], y_train_outer.iloc[te_in_idx]\n",
    "\n",
    "                model = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k))\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_test_inner)\n",
    "                inner_err.append(np.mean(y_pred_inner != y_test_inner))\n",
    "            val_error_knn.append(np.mean(inner_err))\n",
    "        best_idx_knn = np.argmin(val_error_knn)\n",
    "        k_star = k_range[best_idx_knn]\n",
    "\n",
    "        # Retrain on full outer-train with best k, test on outer-test\n",
    "        model_knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=k_star))\n",
    "        model_knn.fit(X_train_outer, y_train_outer)\n",
    "        y_pred_outer_knn = model_knn.predict(X_test_outer)\n",
    "        Etest_knn = np.mean(y_pred_outer_knn != y_test_outer)\n",
    "\n",
    "        #save fold results\n",
    "        fold_results.append({\n",
    "            \"\": i + 1,\n",
    "            \"λ* (log)\": lambda_star,\n",
    "            \"Etest_log\": Etest_log,\n",
    "            \"k* (knn)\": k_star,\n",
    "            \"Etest_knn\": Etest_knn,\n",
    "            \"Etest_base\": Etest_base\n",
    "        })\n",
    "        y_true_all.append(y_test_outer.values)\n",
    "        yhat_log_all.append(y_pred_outer_log)\n",
    "        yhat_knn_all.append(y_pred_outer_knn)\n",
    "        yhat_base_all.append(y_test_pred_base)\n",
    "\n",
    "    y_true_all = np.concatenate(y_true_all)\n",
    "    yhat_log_all = np.concatenate(yhat_log_all)\n",
    "    yhat_knn_all = np.concatenate(yhat_knn_all)\n",
    "    yhat_base_all = np.concatenate(yhat_base_all)\n",
    "\n",
    "    return fold_results, y_true_all, yhat_log_all, yhat_knn_all, yhat_base_all\n",
    "\n",
    "#run two-level CV\n",
    "table, y_true_all, yhat_log_all, yhat_knn_all, yhat_base_all = two_layer_cv(X, y, outer_folds=10, inner_folds=10, random_state=1)\n",
    "table = pd.DataFrame(table)\n",
    "#display results\n",
    "summary = pd.DataFrame({\n",
    "    \"\": [\"logistic\", \"knn\", \"baseline\"],\n",
    "    \"Etest_mean\": [\n",
    "        table[\"Etest_log\"].mean(),\n",
    "        table[\"Etest_knn\"].mean(),\n",
    "        table[\"Etest_base\"].mean()\n",
    "    ],\n",
    "    \"Etest_sd\": [\n",
    "        table[\"Etest_log\"].std(ddof=1),\n",
    "        table[\"Etest_knn\"].std(ddof=1),\n",
    "        table[\"Etest_base\"].std(ddof=1)\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the results\n",
    "print(\"2-layer cross-validation results per fold\")\n",
    "print(table.round(4).to_string(index=False))\n",
    "print(\"\\nSummary\")\n",
    "print(summary.round(4).to_string(index=False))\n",
    "\n",
    "\n",
    "######\n",
    "def mcnemar(y_true, yhatA, yhatB, alpha=0.05):\n",
    "\n",
    "    # Set up the contingency table\n",
    "    nn = np.zeros((2, 2))\n",
    "    # Correctness indicators\n",
    "    cA = yhatA == y_true\n",
    "    cB = yhatB == y_true\n",
    "    # Fill the contingency table\n",
    "    nn[0, 0] = sum([cA[i] * cB[i] for i in range(len(cA))])\n",
    "    # Or a bit smarter: nn[0, 0] = sum(cA & cB)\n",
    "    nn[0, 1] = sum(cA & ~cB)\n",
    "    nn[1, 0] = sum(~cA & cB)\n",
    "    nn[1, 1] = sum(~cA & ~cB)\n",
    "    # get values from the contingency table\n",
    "    n = len(y_true)\n",
    "    n12 = nn[0, 1]\n",
    "    n21 = nn[1, 0]\n",
    "\n",
    "    E_theta = (n12 - n21) / n\n",
    "\n",
    "    Q = (\n",
    "        n**2\n",
    "        * (n + 1)\n",
    "        * (E_theta + 1)\n",
    "        * (1 - E_theta)\n",
    "        / ((n * (n12 + n21) - (n12 - n21) ** 2))\n",
    "    )\n",
    "\n",
    "    f = (E_theta + 1)/2 * (Q - 1)\n",
    "    g = (1 - E_theta)/2 * (Q - 1)\n",
    "\n",
    "    # Calculate confidence interval\n",
    "    CI = tuple(bound * 2 - 1 for bound in st.beta.interval(1 - alpha, a=f, b=g))\n",
    "    # Calculate p-value for the two-sided test using exact binomial test\n",
    "    p = 2 * st.binom.cdf(min([n12, n21]), n=n12 + n21, p=0.5)\n",
    "\n",
    "    print(f\"Result of McNemars test using alpha = {alpha}\\n\")\n",
    "    print(\"Contingency table\")\n",
    "    print(nn, \"\\n\")\n",
    "    if n12 + n21 <= 10:\n",
    "        print(\"Warning, n12+n21 is low: n12+n21=\", (n12 + n21))\n",
    "\n",
    "    print(f\"Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] = {CI[0]:.4f}, {CI[1]:.4f}\\n\")\n",
    "    print(\n",
    "        f\"p-value for two-sided test A and B have same accuracy (exact binomial test): p={p}\\n\"\n",
    "    )\n",
    "\n",
    "    return E_theta, CI, p\n",
    "\n",
    "#run McNemar pairwise comparisons\n",
    "alpha = 0.05\n",
    "print(\"\\nMcNemar Tests, Setup I\")\n",
    "print(\"\\nLogistic vs KNN\")\n",
    "mcnemar(y_true_all, yhat_log_all, yhat_knn_all, alpha)\n",
    "print(\"\\nLogistic vs Baseline\")\n",
    "mcnemar(y_true_all, yhat_log_all, yhat_base_all, alpha)\n",
    "print(\"\\nKNN vs Baseline\")\n",
    "mcnemar(y_true_all, yhat_knn_all, yhat_base_all, alpha)\n"
   ],
   "id": "7557fcae4e811db5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-layer cross-validation results per fold\n",
      "    lambda* (log)  Etest_log  k* (knn)  Etest_knn  Etest_base\n",
      " 1        10.0000     0.2979        15     0.2553      0.3404\n",
      " 2       100.0000     0.2979        11     0.2128      0.3404\n",
      " 3        10.0000     0.2391        15     0.2609      0.3478\n",
      " 4        10.0000     0.3043        13     0.3261      0.3478\n",
      " 5        10.0000     0.2609        15     0.2609      0.3478\n",
      " 6         0.0001     0.3478        15     0.3913      0.3478\n",
      " 7        10.0000     0.2826        15     0.3478      0.3478\n",
      " 8        10.0000     0.2391        15     0.3043      0.3478\n",
      " 9        10.0000     0.2826        11     0.3043      0.3478\n",
      "10        10.0000     0.3043        15     0.2826      0.3478\n",
      "\n",
      "Summary\n",
      "          Etest_mean  Etest_sd\n",
      "logistic      0.2857    0.0330\n",
      "     knn      0.2946    0.0517\n",
      "baseline      0.3463    0.0031\n",
      "\n",
      "--- McNemar Tests (Setup I) ---\n",
      "\n",
      "Logistic vs KNN\n",
      "Result of McNemars test using alpha = 0.05\n",
      "\n",
      "Contingency table\n",
      "[[301.  29.]\n",
      " [ 25. 107.]] \n",
      "\n",
      "Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] = -0.0225, 0.0398\n",
      "\n",
      "p-value for two-sided test A and B have same accuracy (exact binomial test): p=0.6834892282353371\n",
      "\n",
      "\n",
      "Logistic vs Baseline\n",
      "Result of McNemars test using alpha = 0.05\n",
      "\n",
      "Contingency table\n",
      "[[259.  71.]\n",
      " [ 43.  89.]] \n",
      "\n",
      "Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] = 0.0156, 0.1054\n",
      "\n",
      "p-value for two-sided test A and B have same accuracy (exact binomial test): p=0.011118593448764544\n",
      "\n",
      "\n",
      "KNN vs Baseline\n",
      "Result of McNemars test using alpha = 0.05\n",
      "\n",
      "Contingency table\n",
      "[[263.  63.]\n",
      " [ 39.  97.]] \n",
      "\n",
      "Approximate 1-alpha confidence interval of theta: [thetaL,thetaU] = 0.0094, 0.0944\n",
      "\n",
      "p-value for two-sided test A and B have same accuracy (exact binomial test): p=0.022297661824692908\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.05194805194805195),\n",
       " (np.float64(0.009368875464850213), np.float64(0.09443422992047412)),\n",
       " np.float64(0.022297661824692908))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "fa1a16aa",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "id": "f1c4dace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T17:22:16.614378Z",
     "start_time": "2025-11-09T17:22:16.611224Z"
    }
   },
   "source": [
    "def regularize_data(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df['famhist'] = df['famhist'].map({'Present': 1, 'Absent': 0})\n",
    "    df = (df-df.mean()) / df.std()\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "468668c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T17:22:16.657503Z",
     "start_time": "2025-11-09T17:22:16.640909Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def logistic_regression_classification():\n",
    "    threshold = 0.5\n",
    "\n",
    "    df = regularize_data(\"data.csv\")\n",
    "    y = (df[\"chd\"]>threshold).astype(int)\n",
    "    X = df.drop(columns=[\"chd\", \"row.names\"])\n",
    "\n",
    "\n",
    "    lambda_val = 0.1 #pode ter que ser mudado porque depende do ponto 4\n",
    "\n",
    "\n",
    "    C_val = 1 / lambda_val\n",
    "    logreg_model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=100,\n",
    "        solver=\"lbfgs\",\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    logreg_model.fit(X, y)\n",
    "\n",
    "    print(\"Bias:\", logreg_model.intercept_)\n",
    "    print(\"Features:\", X.columns.tolist())\n",
    "    print(\"Coefficients for each feature:\", logreg_model.coef_)\n",
    "\n",
    "\n",
    "    y_pred = logreg_model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    # mse\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "    print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Logistic Regression MSE: {mse:.4f}\")\n",
    "\n",
    "logistic_regression_classification()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: [-0.87863012]\n",
      "Features: ['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity', 'alcohol', 'age']\n",
      "Coefficients for each feature: [[ 0.13321451  0.36438477  0.36031357  0.14263364  0.45636594  0.38864065\n",
      "  -0.26353865  0.00313647  0.66182163]]\n",
      "Logistic Regression Accuracy: 0.7338\n",
      "Logistic Regression MSE: 0.2662\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
